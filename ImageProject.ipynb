{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4a391-4b1d-45de-89f1-1221e98c417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "import re\n",
    "import time\n",
    "from PIL import Image, ExifTags\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(filename='error_log.log', level=logging.ERROR)\n",
    "\n",
    "# Define compression thresholds and size limits\n",
    "small_size_limit = 3 * 1024 * 1024  # 3 MB\n",
    "medium_size_limit = 10 * 1024 * 1024  # 10 MB\n",
    "target_size_limit = 6 * 1024 * 1024  # 6 MB\n",
    "min_image_size = 0.05 * 1024 * 1024  # 0.05 MB (50 KB) - Minimum valid image size\n",
    "allowed_extensions = ['.jpg', '.jpeg', '.png', '.tiff', '.bmp', '.gif']\n",
    "\n",
    "# Directory for corrupted files and \"NoDate\" folder\n",
    "corrupted_folder = \"CorruptedImages\"\n",
    "nodate_folder = \"NoDate\"\n",
    "\n",
    "# Ensure directories for corrupted files and NoDate exist\n",
    "os.makedirs(corrupted_folder, exist_ok=True)\n",
    "os.makedirs(nodate_folder, exist_ok=True)\n",
    "\n",
    "# Function to extract the year from EXIF data\n",
    "def get_image_year(file_path):\n",
    "    try:\n",
    "        img = Image.open(file_path)\n",
    "        exif_data = img._getexif()\n",
    "        if exif_data:\n",
    "            for tag, value in exif_data.items():\n",
    "                decoded_tag = ExifTags.TAGS.get(tag, tag)\n",
    "                if decoded_tag == 'DateTimeOriginal':\n",
    "                    year = value.split(\":\")[0]\n",
    "                    if validate_year(year):\n",
    "                        return year\n",
    "    except Exception as e:\n",
    "        logging.error(f\"EXIF extraction failed for {file_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Function to extract the year from a filename by matching common patterns\n",
    "def extract_year_from_filename(filename):\n",
    "    date_patterns = [\n",
    "        r\"IMG[_-](\\d{4})(\\d{2})(\\d{2})\",   # Matches IMG_YYYYMMDD or IMG-YYYYMMDD\n",
    "        r\"(\\d{4})[-_](\\d{2})[-_](\\d{2})\",  # Matches YYYY-MM-DD or YYYY_MM_DD\n",
    "        r\"(\\d{8})\",                        # Matches YYYYMMDD\n",
    "        r\"(\\d{4})\"                         # Matches any standalone year (fallback)\n",
    "    ]\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            if validate_year(year):\n",
    "                return year\n",
    "    return None  # Return None if no valid date is found\n",
    "\n",
    "# Function to validate the extracted year (only accept years between 2000 and 2030)\n",
    "def validate_year(year):\n",
    "    try:\n",
    "        year = int(year)\n",
    "        if 2000 <= year <= 2030:\n",
    "            return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "# Check if the file is a valid image and is larger than 0.05MB using PIL\n",
    "def is_image(file_path):\n",
    "    try:\n",
    "        if os.path.getsize(file_path) < min_image_size:\n",
    "            return False  # Ignore images smaller than 0.05MB\n",
    "        img = Image.open(file_path)\n",
    "        img.verify()  # This will raise an exception if the file is not an image\n",
    "        return True\n",
    "    except (IOError, SyntaxError):\n",
    "        return False\n",
    "\n",
    "# Calculate the hash of the image to avoid duplicates\n",
    "def calculate_hash(file_path):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "# Fix image orientation based on EXIF data more reliably using Pillow's `ImageOps` module\n",
    "def fix_orientation(img):\n",
    "    try:\n",
    "        exif = img._getexif()\n",
    "        if exif is not None:\n",
    "            orientation_key = 274  # Exif key for orientation\n",
    "            if orientation_key in exif:\n",
    "                orientation = exif[orientation_key]\n",
    "\n",
    "                # Rotate according to EXIF orientation\n",
    "                if orientation == 3:\n",
    "                    img = img.rotate(180, expand=True)\n",
    "                elif orientation == 6:\n",
    "                    img = img.rotate(270, expand=True)\n",
    "                elif orientation == 8:\n",
    "                    img = img.rotate(90, expand=True)\n",
    "    except (AttributeError, KeyError, IndexError):\n",
    "        pass  # If the image has no EXIF orientation data, do nothing\n",
    "    return img\n",
    "\n",
    "# Compress the image based on its size, preserve EXIF, and store it in the destination folder\n",
    "def compress_image(image_path, dest_dir, target_size_mb=6):\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Fix the image orientation based on EXIF data\n",
    "    img = fix_orientation(img)\n",
    "\n",
    "    # Extract EXIF data\n",
    "    exif_data = img.info['exif'] if 'exif' in img.info else None\n",
    "\n",
    "    quality = 95  # Start with high quality\n",
    "    img_format = img.format if img.format != 'JPEG' else 'JPEG'\n",
    "\n",
    "    # Create the compressed image path inside the destination directory\n",
    "    output_path = os.path.join(dest_dir, os.path.basename(image_path))\n",
    "\n",
    "    # Save the image with EXIF data preserved\n",
    "    img.save(output_path, format=img_format, quality=quality, exif=exif_data)\n",
    "\n",
    "    while os.path.getsize(output_path) > target_size_mb * 1024 * 1024:\n",
    "        quality -= 10\n",
    "        if quality < 10:\n",
    "            break  # Stop if quality becomes too low\n",
    "        img.save(output_path, format=img_format, quality=quality, exif=exif_data)\n",
    "\n",
    "    return output_path  # Return the path of the compressed image\n",
    "\n",
    "# Function to flatten folder structure if it contains only one subfolder\n",
    "def flatten_single_subfolder_folders(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # If the folder has only one directory and no files, flatten it\n",
    "        if len(dirs) == 1 and not files:\n",
    "            single_dir = dirs[0]\n",
    "            src = os.path.join(root, single_dir)\n",
    "            parent = os.path.dirname(root)\n",
    "            new_path = os.path.join(parent, os.path.basename(single_dir))\n",
    "            if not os.path.exists(new_path):\n",
    "                shutil.move(src, parent)  # Move the subfolder up\n",
    "                os.rmdir(root)  # Remove the empty intermediate folder\n",
    "\n",
    "# Function to remove empty folders after processing\n",
    "def remove_empty_folders(destination_dir):\n",
    "    for root, dirs, files in os.walk(destination_dir, topdown=False):\n",
    "        if not files and not dirs:  # If the folder is empty\n",
    "            os.rmdir(root)\n",
    "\n",
    "# Function to organize images into year-based folders (copy instead of move)\n",
    "def organize_images(source_dir, destination_dir):\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "    processed_images = set()\n",
    "    if os.path.exists('processed_images.txt'):\n",
    "        with open('processed_images.txt', 'r') as f:\n",
    "            processed_images.update(f.read().splitlines())\n",
    "\n",
    "    # Count all images in all folders for a single progress bar\n",
    "    total_images = sum([len(files) for r, d, files in os.walk(source_dir) if files])\n",
    "\n",
    "    image_count = 0\n",
    "    total_size_before = 0\n",
    "    total_size_after = 0\n",
    "\n",
    "    with open('processed_images.txt', 'a') as processed_file, tqdm(total=total_images, desc=\"Processing images\") as pbar:\n",
    "        for root, dirs, files in os.walk(source_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if not is_image(file_path) or file_path in processed_images:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                image_count += 1\n",
    "                total_size_before += os.path.getsize(file_path)\n",
    "\n",
    "                # Calculate the hash to avoid duplicate processing\n",
    "                file_hash = calculate_hash(file_path)\n",
    "                if file_hash in processed_images:\n",
    "                    pbar.update(1)\n",
    "                    continue  # Skip duplicate images\n",
    "\n",
    "                # Extract the year from EXIF data or filename, validate the year\n",
    "                year = get_image_year(file_path) or extract_year_from_filename(file) or \"NoDate\"\n",
    "\n",
    "                # Define destination directory with year and folder structure\n",
    "                relative_path = os.path.relpath(root, source_dir)\n",
    "                dest_dir = os.path.join(destination_dir, year, relative_path)\n",
    "                os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "                # Determine compression strategy based on size\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                if file_size > small_size_limit:\n",
    "                    try:\n",
    "                        # Compress and save directly in the destination folder (no extra copy step)\n",
    "                        compress_image(file_path, dest_dir)\n",
    "                        total_size_after += os.path.getsize(os.path.join(dest_dir, file))\n",
    "                    except PermissionError as e:\n",
    "                        logging.error(f\"Permission denied for {file_path}: {e}\")\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Compression failed for {file_path}: {e}\")\n",
    "                        # Retry copying the file to the corrupted folder\n",
    "                        retries = 3\n",
    "                        for attempt in range(retries):\n",
    "                            try:\n",
    "                                shutil.copy2(file_path, os.path.join(corrupted_folder, file))\n",
    "                                break\n",
    "                            except PermissionError as e:\n",
    "                                logging.error(f\"Retry {attempt+1}/{retries} failed for {file_path}: {e}\")\n",
    "                                time.sleep(1)  # Sleep for a second and retry\n",
    "                                if attempt == retries - 1:\n",
    "                                    logging.error(f\"Copying to CorruptedImages failed for {file_path}: {e}\")\n",
    "                                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        shutil.copy2(file_path, os.path.join(dest_dir, file))  # Copy without compression\n",
    "                    except PermissionError as e:\n",
    "                        logging.error(f\"Permission denied for {file_path}: {e}\")\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                processed_file.write(f\"{file_hash}\\n\")\n",
    "                processed_images.add(file_hash)\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Flatten folder structure after all files are processed\n",
    "        flatten_single_subfolder_folders(destination_dir)\n",
    "\n",
    "    # Remove empty folders\n",
    "    remove_empty_folders(destination_dir)\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"Processed {image_count} images.\")\n",
    "    print(f\"Total size before: {total_size_before / (1024 * 1024)} MB\")\n",
    "    print(f\"Total size after: {total_size_after / (1024 * 1024)} MB\")\n",
    "    \n",
    "# Example usage\n",
    "source_directory = r\"E:\\Pictures\\\"\n",
    "destination_directory = r\"C:\\Users\\XYZ\\OneDrive\\Media\"\n",
    "\n",
    "organize_images(source_directory, destination_directory)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
